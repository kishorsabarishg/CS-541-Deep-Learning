# -*- coding: utf-8 -*-
"""Homework2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hZl4jbG2unNKeT2R8vVQ9BSFjoNLtlV9
"""

import numpy as np
import time

start = time.time()

def SGD(X,Y,Y_hat,alpha,W):
    gradient_w = np.dot(X.T,(Y_hat - Y))/X.shape[0] + (alpha/X.shape[0])*W
    #print(gradient_w)
    gradient_b = np.mean(Y_hat - Y)
    
    return gradient_w, gradient_b

def FMSE(X,Y,W,B,Alpha):
    fmse = (1/(2*X.shape[0]))*(np.dot((np.dot(X,W) - Y).transpose(), np.dot(X,W) - Y)) + (alpha/(2*X.shape[0]))*(np.dot(W.T,W))
    return fmse

best_fmse = 10000000
best_epoch = 0
best_batch = 0
best_lr = 0
best_alpha = 0
best_w = 0
best_b = 0

X = np.load("age_regression_Xtr.npy")
Y = np.load("age_regression_ytr.npy")

X = X.reshape(X.shape[0],-1)
Y = Y.reshape(-1,1)

X_test = np.load("age_regression_Xte.npy")
Y_test = np.load("age_regression_yte.npy")

X_test = X.reshape(X.shape[0],-1)
Y_test = Y.reshape(-1,1)


split = 0.8*X.shape[0]
split = int(split)

indices = np.random.permutation(X.shape[0])
#print(indices)

training_idx = indices[:split]
val_idx = indices[split:]

#print(split)

X_train = X[training_idx,:]
Y_train = Y[training_idx,:]
X_val = X[val_idx,:]
Y_val = Y[val_idx,:]

#print(X_train.shape,X_val.shape)

Learning_rate = [0.001, 0.005, 0.01, 0.0005]
Batch_size = [128,256,512,1024]
Epochs = [50,100,200,400]
Reg_con = [0.01, 0.2, 0.5, 0.005]

#w = np.random.randn(X_train.shape[1],1) #weight matrix
#b = np.random.randn(1)
#print(b.shape)
#print(w.shape)

#count = 0

for epsilon in Learning_rate:
    for epoch in Epochs:
        for batch in Batch_size:
            for alpha in Reg_con:
                
                w = np.random.rand(X_train.shape[1],1) #weight matrix
                b = np.random.rand(1)
                
                #print(epochs)
                for i in range(epoch):
                  #print("Epsilon:",epsilon,"Epoch:",epoch,"Batch Size:",batch,"Alpha:",alpha)
                    
                        
                      #count += 1
                      
                  index = np.random.choice(np.arange(X_train.shape[0]), size = batch, replace = False )
                  X_mini = X_train[index,:]
                  Y_mini = Y_train[index]
                        
                  Y_hat = np.dot(X_mini,w) + b
                      #print(Y_mini.shape)
                        
                  gradient_w, gradient_b = SGD(X_mini, Y_mini, Y_hat, alpha, w)

                        
                  w = w - epsilon*gradient_w
                  b = b - epsilon*gradient_b
                
                  
                  training_cost = FMSE(X_mini, Y_mini, w,b, alpha)
                
                validation_cost = FMSE(X_val,Y_val,w,b,alpha)
                
                #print(validation_cost)
                
                if validation_cost < best_fmse:
                    best_epoch = epoch
                    best_lr = epsilon
                    best_batch = batch
                    best_alpha = alpha
                    best_w = w
                    best_b = b
                    best_fmse = validation_cost
                    
print("Best Epochs:",best_epoch,"\nBest Learning Rate:",best_lr,"\nBest Batch Size:",best_batch,"\nBest Regularization Rate:",best_alpha,"\nBest Validaation Loss:",best_fmse)
testing_cost = FMSE(X_test, Y_test, best_w, best_b, 0)
print("\nTesting_FMSE_LOSS:",testing_cost)


end = time.time()

tt = end - start

print(tt)